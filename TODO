* Testing
** Document Set
using hamlet for lucene tests, solr config + fake docs for solr
** Query Set
evolving
** Performance Measurement
* Basic Implementation
** Lucene-only implementation 
This will be better for testing, for performance optimization, etc. and should help us keep a cleaner API
*** implement querying from LuXPath?
** Filtering in Solr
This is done to first order
*** Handle documents that match the query, but don't match the xpath
*** Handle result sets that require multiple pages of documents
*** Handle documents that return multiple nodes
** Pagination
when start > 1, change doc-start = 1 and scan forward
*** unless the query is minimal
then set doc-start = start
**** minimal document results, or minimal counts only
can skip XPath step entirely
*** cache result navigation info in Solr
if we've computed which document contains xpath X position A, and a request
comes in for xpath X position B, and B > A, we can start at A and scan
forward.  This can be a very small cache.  The idea is that one consumer is
skipping around in a result set.  The main case is to optimize is scanning
through an entire large result set.
** Counting
count (//foo)
*** need to handle functions at outermost scope
expressions like (//foo)[1] too
** cleanup API, rename classes
This is better now - but still not sure about LuXPath
once we have a few implementations done, revisit the naming
* Solr integration
** namespaces
*** in QParser config
*** in query localnames
** marshal XML types
preserve xpath result types (nodes and xdm atomic types) across Solr's
comms This is a bit better now.  But too much parsing/unparsing.
** configure XPathSearchComponent, XPathQParser, LuxProcessor
*** xml field name
*** namespace-aware
for parser and update processor
* Administrivia
** svn
** domain/namespace
lux.* is unavailable
luxpath.net/org are available
** distribution
* Fancier XPath
** external variable binding
** index some functions
*** root()
*** count()
across multiple documents
*** base-uri(), node-uri()
** optimize some cases
eg where a sequence will always return one result per document, and we know
that we can match the documents exactly by query.  For example:
/descendant::foo[2] (or any constant integer or last() in the predicate, or
position() = same).
* Highlighting
* Advanced Indexing
** index paths
** index text and values
parameterize existing indexing classes
** index XML structure
* desiderata
** XML namespaces
Don't pollute queries with horrible namespace declarations.  Allow
namespaces to be declared in configuration only.  Also allow namespace
support to be disabled, in which case index QNames directly.
** sorting
only document order for now; XPath doesn't allow for any other sort order,
but of course we would like to sort the documents too...
* PLAN
Make small steps so we can progress without being overwhelmed, measure each
step carefully before moving on.
** ideas
this doc and xml-indexer.txt
** infrastructure, tools
jaxen, solr
** structure, naming, project shell
we have a project shell and some preliminary names
*** package name solrx, org.solrx, (org.) lux, lucx, lucex, lucenex?
Keep in mind the idea of providing implementations as raw classes w/o solr
dependencies.
*** class names falling into place
** the bare minimum
*** query translator 
**** unit tests
Basic query translator tests OK
**** separate attributes from elements
decide whether to use combined element/attribute indexes here
**** as a QueryParserPlugin
later - not part of the minimum
*** indexer
**** design
single-pass parse using Woodstox that accumulates stack frames
encompassing: One frame per XML node (text and attrs as leaves could be
handled w/out frames?)  each frame should include char offsets in the
original XML, an id (hierarchical), a parent id, a doc id.
***** QNames
***** QName paths
implicit in a stack of QNames
***** node words / values
this is easy for attributes; for elements need to decide about child
boundaries and whether values should be truncated.  Assuming phrase-through
config like ML, but still include child words in parent value so they can
have the same meaning as XPath string value.  However we should truncate.
There's no sense in string value > 200 chars is there?  If we need to
retrieve a node we can reconstruct its text from the document and the node
offsets.  So then the only purpose for the values is to search them.  But
nobody wants to search a long string as a value.
***** XPath indexes
These really do have to be computed in a second pass.  If we are going to
create a tree anyway, we could choose to just traverse that in order to
handle the other indexing too.
***** See org.jdom.input.StAXBuilder
Our reader would need to absorb and merge with this in order to maintain a
single-pass approach.
**** impl plan
***** combined StAXBuilder/XMLCharFilter/NodeIndexer.  
Something weird though - as a CharFilter, XmlCharFilter gets invoked by
lucene as part of analysis of a specific field value.  It doesn't know from
Documents, Fields, etc.  It just gets a Reader - that's it.
***** eliminate CharFilter
Perhaps the best approach here would be to create a different
beast.  Build an LuXmlReader that (1) extracts text, preserving character
offsets and (2) generates a JDOM.  Then we wouldn't need the char filter.
Then a separate indexer generates fields from the JDOM and the text.

Actually a neat idea would be an object model derived from the original
character buffer.  We'd represent each element by a bunch of pointers; the
text nodes and attribute values would be references to the buffer.
But this optimization is truly premature.

However this doesn't work b/c of entities.  The only way to model them is
with offsets, which are a CharFilter feature.  And inserting phrase
boundaries at element boundaries becomes tricky too.
***** two-pass system
One pass for indexing nodes, and another for handling the whole document -
so we can do highlighting.
**** unit tests
**** as an UpdateProcessor
** basic implementation complete 
*** integration test framework
loads documents, evaluates query, timings
** a complete working implementation
* implementation notes on using Jaxen
consider saxon which provides XPath 2.0? Maybe for version 2...  It turns
out Jaxen <> javax.xml.xpath, which is bundled w/Oracle JRE now.  But JRE
version doesn't expose its parse tree, so is not usable.
** extend BaseXPath and wrap another XPath implementation (eg JDOM, dom4j, etc)
*** getContext()
Allow the use of a database collection as a context (all docs for now, but
allow for extending to a restriction by document query)
*** selectNodesForContext()
When passed a database context, do our stuff, otherwise delegate
** extend DefaultXPathFactory
We want to: (1) generate a lucene query, and (2) know whether it is exact.
Wrap all of the exprs, providing an asLuceneQuery(Query) method.  It needs
to be able to contribute to a query, or not, and also indicate whether it
is fully evaluated by its contribution; say it returns a boolean. Sibling
axes could contribute, but only partially, if say we didn't really
accurately index that info, but we would find docs that have both nodes a
and b in a::following-sibling:b.  For example, we might only be able to do
b[../a] without the order constraint, or maybe only /[.//b][.//a].
*** have to use visitor
I was intending to wrap the Expr classes as described above, providing my
own XPathHandler wrapping jaxen's JaxenHandler, but unfortunately BaseXPath
doesn't provide a constructor that would allow you to do that.

So instead we can just crawl over the parse tree, converting it.
** execute the query
If it's exact, simply return the results.  Otherwise, the results must be
from the same space as the provided context for the query.  In all but one
of the schemes, we always return whole documents from the Lucene query.  In
the other case, we may return documents, elements or attributes, and if
these *are* the results, we're done.
** process the results if needed
Then pass each result in succession as a Context to the wrapped XPath
implementation, and return the combined result set, possibly paginated.
