* features
** distributed search (and indexing)
*** document identity and ordering
We have been using docid as an ordering mechanism until now.  Cloud
complicates this because the same docid may exist on multiple shards.  But
this problem merely exposed an underlying problem we already had w/Solr:

XQueries involving multiple Solr queries may get documents on either side
of a commit (maybe a soft commit).  In this case, documents may be added or
deleted, and existing documents may have their ids changed.  This causes
problems with joins involving document or node identity.

The only way to guarantee correct XQuery semantics in the face of updates
for such queries is to provide a stable global ordering for all documents.
We could do that easily enough using our existing unique key, uri.  However
we face an additional hurdle in that Saxon requires a numeric document id
for every document, which is what it uses to enforce document order.  Our
unique keys (uris) are strings though, and it's not possible to assign them
unique 64-bit ints; we'd also need to preserve the natural string order
since our id order must correspond with the order of our unique key.

Let's abandon the idea of using uri for ordering and consider the idea of
providing our own unique integer key.  This would require us to build an
atomically-updatable persistent id sequence somewhere and to assign ids on
insert.

But we're interested in providing a Solr Cloud implementation, which means
updates can come to multiple nodes simultaneously.  We would need to handle
coordination of updates to the atomic id sequence across any number of
nodes, and what this would amount to basically, would be adding a
transaction mechanism to Solr Cloud, and that seems like a tall order.

Now instead let's consider the consequences of relaxing the XQuery language
constraint.  We can concoct an integer id that mimics the "unordered" Solr
Cloud document order by merging a shard id and the (shard-local) document
id: (shard_id << 32) | docid.

If a commit occurs during the evaluation of a multi-query XQuery, we could
end up with:

a) an id collision: a different document in query B re-uses the id of a
document in query A.  This could cause us to retrieve document A from the
cache.

b) an id change: the same document in queries A and B with different ids:
these will appear as distinct documents, albeit with the same content.

Essentially, we will get incorrect results from queries of this sort.  And
preventing this is very difficult.  Our only real option is to provide some
sort of error detection and recovery.  We can easily detect id changes by
storing a per-query (ie short-lived) two-way map id <=> uri.  Then if we
detect either an existing id with a different uri, or an existing uri with
a different id, we can throw an error and attempt to recover.

Probably the only thing we can really do to recover if the document
ordering changes mid-query, is to restart the query and try again.  We can
do this without the user noticing anything other than a slowdown.  If
enough failures occur in the same query we would eventually throw an error.

So how bad is this?  Querying documents while they're being updated may
happen during the course of some kind of batch data update, but queries
that depend on node and document identity in XQuery are fairly unusual.
Oftentimes they can be rewritten as simpler predicates.  The worst thing
might be degraded performance that goes unnnoticed due to frequent query
resets.  We should log these at WARN level so users are aware of their
existence.

Implementation notes:

As far as maintaining the map goes: we already keep an id->document
(XdmNode) map, and the document includes both its uri and its id
(XdmNode.getDocumentNumber()).  We don't have a uri->document map though;
we would probably have to add that.  But the existing map (nodeCache) is an
LRUCache that is limited in size, so it wouldn't guarantee correctness once
a node was expired from the cache.  This is a guard againt blowing out the
heap, since it includes entire documents. So probably we would need to add
both a uri->id map and an id->uri map.

OK that's out the window now since we don't actually get docids back from
Solr, and hacking it to return them looks like a real PITA, aside from
which it's leaking internal stuff etc.

So looking into generating a unique long key: we could generate a fairly
unique key using uri.hashval() * time() * random() (or seed random with the
foregoing), but then we have to look up that id and insert only if it
doesn't already exist *in solr*.  But this isn't truly an atomic operation:
in theory someone else can insert our random key.  Unlikely, but this also
has the drawback of having to run a query across the cluster before every
update.  Another idea: we can just be optimistic about the uniqueness -
someone on SO calculated the likelihood of a collision in a 10M document
collection to be about 1/10M.  So that's quite unlikely, and then it's not
as if document integrity is compromised if we have a collision - just
document order in some unusual queries.

In the short term we can just assign ids sequentially for each query and
not worry about this, I guess.

So SignatureUpdateProcessor creates a (64-bit!) hash based on a document's
content.  It was designed for dedupe processing.  But if we simply feed it
the uri, it can serve as a source of a 64 bit integer -- we need to ensure
that the integer sorts the same way as the signature.  Of course there is a
potential for hash collisions.  A nice fix would be to mix in a random
number or timestamp field to the hash: that way if an insert fails due to
non-identical signatures, we just try again.

uniqueness: lucene deletes the document matching the "updateTerm" which can
either be an id field (like uri), or the signature field. In our case we
would use uri, I think.  Then, if idField != signatureField, the
updatehandler also checks for uniqueness of the signature, but what it does
is to delete any existing matching documents!  So this is not really helpful...

Perhaps another approach would be to extend SignatureUpdateProcessor to
query for the existence of a hash match with a different id (uri).  But
will this work with clouded updates?  Does this processor work at all
w/shards?  It appears to check for dupes only in a local index.

*** search
This is complicated b/c Saxon wants to pull results, and Solr, to push
them.  We had a similar issue in the single-node search with Lucene, but
managed to overcome by implementing our own pull-style Lucene searcher.
It's not truly pull-style when the results are sorted (by relevance score
or by value).  In that case we buffer the first N results (docids only),
and if more get pulled, we fetch the next batch.

In Solr Cloud, the RequestHandler orchestrates communication with a number
of shards, and with a list of SearchComponents.  It calls each component
once (distributedProcess()) in each phase.  Communication between these
components is mediated by the ResponseBuilder, which holds information
about the current state of the request.

Ultimately what we need to do is evaluate the xquery (control passes to
Saxon), and then when a search function is called, initiate the entire
distributed search process, following a similar procedure as we currently
do for sorted results in LuxSearcher: fetch the top N from each shard, and
repeat if needed.

This suggests tinkering with the RequestHandler.  XQueryComponent
can *call* RequestHandler.handleRequestBody and then return STAGE_DONE.
Now typically SearchComponents do not maintain a pointer to the handler
that owns them.  We maintain a (per-core) singleton, SolrIndexConfig, which
is available to the XQueryComponent via the core's plugin registry.
Probably we can grab onto the RequestHandler from there?  There can be
multiple request handlers though, and we want to make sure to use the same
one: so we can iterate over all the request handlers in the core, and
iterate over all the components in each handler until we find the one we're
attached to.  Alternatively we could subclass the RequestHandler and inform
the component (if it's an XQueryComponent)

*** test distributed search

*** test distributed indexing

I believe this should just work
** directory (uri component) index
also link to collection()?
** fragmentation 
use Lucene's block join indexing
** binary documents
I think this is working now - let's measure and document the improvement.
** execute app from index
* optimization
cache translated AbstractExpressions for indexed XPaths somewhere.
IndexConfiguration seems like the right place, but it doesn't have any
access to an XPathCompiler (nor a SaxonTranslator).  These are available to
XmlIndexer, Compiler, and (for the translator) PathOptimizer.
** not() optimization:
*** invert the query:
count(collection()[not(.//pubdate)])
  optimize as:
count(collection()) - count(collection()[.//pubdate])
*** optimize not(empty(X)) -> exists(X), and not(exists(X)) -> empty(X)
** handle variables in comparisons
at least if they're atomic?
** replace query stack
Store queries with the abstract expressions that generate them.  The stack
abstraction is opaque, and fragile.  To make it work we would need
something that enforces balanced pushing and popping.  Our convention is
that every expression pushes a single query and pops as many queries as it
has sub-expressions.  I spent a whole day tracking down a bug in the way we
were handling queries for FLWOR clauses, because they don't really fit the
stack model very well.
** optimize the Optimizer 
It currently generates an entire translated expression tree for every
subtree it attempts to optimize so that we can figure out if that subtree
is ordered or not.  We could possibly cache a translated tree and walk it
in parallel?  Or link subexpressions to it with some kind of map?
*** cache compiled (optimized) expressions
Where does such a cache belong?
** Path occurrence indexes 
Suppose that instead of phrases, we indexed all paths, as "keywords", ie
untokenized; the position would be the depth-first sequence, so like:

a a/b a/b/c a/b/c a/z a/z/@id 

except this has the real drawback that you are almost always interested in the *tail end*, so let's reverse: 

a b/a c/b/a c/b/a z/a @id/z/a

this lets you easily search for a full path, or a sub-path preceded by //,
which becomes a prefix query.

TermDocs iterates over term frequency per doc; with that it should be 

TermFreqValueSource counts term freq; use it in a FunctionQuery??

FunctionQuery creates an AllScorer; we would want it to use instead a 

Scorer like the one returned by FunctionValues.getRangeScorer

Solr FunctionRangeQuery does what we want using Solr's ValueSourceRangeFilter

so: new FunctionRangeQuery(new ValueSourceRangeFilter (new TermFreqValueSource (field, val, indexedField, indexedBytes), min, max, includemin, includemax))

(note field, val seem to be basically unused in TFVS? )

** pre-evaluate some functions, like aggregates:

In cases where we can do this (argument expression is evaluated in the
QueryContext, ie there is no current node, or the argument is the
collection() function), rewrite/wrap/intercept the function to an internal
method (lux:count) that we implement by recursing into a nested query/eval.

Perform static analysis of the argument expression(s) to see if they can be
retrieved minimally by query.  If we can prove that the db query result
count will equal the expression result count, then we can replace the
count() expression with an (indexed, more efficient) query-count()
expression.  exists() and not() can be computed as count()>0 and count()=0,
and we can short-circuit evaluation in the result collector.

This could also hold for max/min when we have an appropriate field.  Maybe
we can even help out w/aggregates like sum/avg?

More complex would be something like: count(//a) + count(//b) ?

*** root()
*** count()
across multiple documents
*** node-uri(), document-uri()
** optimize pagination when we know that #count=#estimate
optimize subsequence when we can - eg the first arg, the sequence, is
entirely indexed.  And even when it's not, make sure we don't iterate on
beyond where we need to - asserting the results are sorted properly.  We're
somewhat limited in our ability to do this right now since we rely on Saxon
to make judgements about sorting, and it doesn't always optimize as
agressively as we would like, at least not in the saxon-HE version.  It
works in some simple cases, though.
* app-server
** documentation review
** integration test
Enhance the integration test so it checks that the demo runs
** http header control
redirect, 404, error, etc
** passing current uri
We currently send Solr the translated path - so it can read the resource,
but it doesn't have the original url, which it would be nice to have so we
can give it to the application
* demo
* coverage 
SaxonTranslator: 85,87,90
** namespace lookup failure during document ordering analysis
check w/XQTS
** operators:
negate
** getTypeDescription 
with ?
** types
xs:hexBinary, xs:base64Binary, comment(), empty()
** variable binding to constant??
try w/XQTS
** override fn, local and xs namespace prefixes
** funky flwor expression with leading where clause?
see exprFor(FLWORExpression)
try w/XQTS
* performance evaluation
** tests
StressTest - compare w/ML, Solr, exist
** Compare indexing speed
** Compare query/eval speed
show impact of parsing, document retrieval?
** Test performance on XQTS with Saxon alone, and with Lux/Saxon
show impact of Lux on compilation speed We did this.  The overall slowdown
on XQTS is a bit depressing: running the entire test suite with Lux enabled
takes about 50% longer than without it.  However this seems largely to be
due to the effect of some tests which, when compiled, lead to expression
trees that are much larger than the original xquery form.  The rule of
thumb seems to be that you can expect compilation time to double.  In XQTS,
compilation time dominates the test (after simply loading test data).

* absolute path in function context
Saxon treats this as a compile-time error, so there's really not much we
can do about it short of introducing a different parser.
* compatibility w/vanilla Saxon
We'd like to be able to use Saxon PE/EE.  Currently if you use a licensed
Configuration, you lose out on some nice performance optimizations for some
queries that search large numbers of documents.
** purchase Saxon-PE
** Don't extend Configuration
We currently do this so we can use a custom FunctionLibrary
to ensure that search results are regarded as properly in document order.
We could avoid the need for this if we implement a CollectionURIResolver
that handles a few special schemes (like lux:) since results from
collection() are considered to be in document order.
*** DocumentSorter
We also prevent additional sorting of the entire result set in some cases
we can determine it to be unnecessary by providing a custom Optimizer that overrides
Optimizer.makeConditionalDocumentSorter.  This happens, eg when evaluating
a path like exists(//foo//bar).  There is no way around this in Saxon HE,
and it is a significant performance whack for many queries.  We need to test if Saxon PE
and/or EE do a better job of optimization here.
* HTML/HTML5
try reading files using James Clark's fixup parser?  Tag Soup, more likely
- is this built in to Saxon?
* improve compilation speed
XQTS tests run substantially slower with Lux "optimization" since
compilation speed is at least 2x for every expression, and this seems to
dominate the test speed.
** Use Saxon CodeInjector to manipulate the expression tree
Unfortunately this isn't quite feasible because CodeInjector doesn't get
called at enough entry points.  It seemed close?
* word index followthrough
** provide hooks for custom analyzer in QNameAnalyzer/QNameTokenStream
Apply custom analysis chain to each text node, and then wrap in
QNameFilter.
** alternate fields
with different analysis, different element exclusions, different attribute
inclusion rules.  For example there should be a field that includes only
attributes.
* testing for correctness
** develop an XQTS database query test
Ignore queries that use the emptydocument as input Run the other queries
against the entire test set as context: ie against the sequence of
documents that is collection(xqts) and then run using Lux.  Make sure the
results are consistent; compare timing.

I've done this, but it's hard to prove that we're getting the same results
back since the order in which documents are returned from collection()
seems to be a bit unpredictable in Saxon?
** standard function library untested
A bunch of functions are untested.  We don't know if we're doing the right
thing in terms of query generation based on their args.  See
PathOptimizer.fnArgParity
** search test2
refactor and merge w/search test - check opto facts against actual behavior
** search test all index types
include name-index only test
