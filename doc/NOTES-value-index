
It seems as if one should use the hashing approach for value indexes?  At
least in terms of space savings, it's significant, and if all you want it
for is searching for element- or attribute-value matches.  In conjunction
with paths, that would be pretty good.

What advantages would there be to keeping the QName path-value indexes
expanded as multiple tokens?  You would end up with distinct tokens for
each *instance* of a given QName or path meaning in theory you could count
them with a query.  EG: you could quickly find
collection()[./descendant::foo[2]] (how?)  with a qname-value index by
finding all docs with the term and filtering by term freq - this looks kind
of hairy; I don't see term frequency filtering as a first-class query
anywhere

Text Indexing 

Single field: here each word is indexed as QName:word.  Value-matching is
possible if we include terminals for each node (start and end tokens that
mark the node boundaries).  Within-node and-queries are also sort-of
possible, at least to some degree of approximation, using position-based
queries with a largish slop.

Multi-field: creates dynamic fields named according to QName.  This works
essentially the same as single field (since Lucene internally indexes terms
as field:value), but avoids any confusion about delimiting the name/value
boundary.  It runs into a little messiness in terms of managing the Solr
schema, though?

Start and end tokens present a challenge; ideally we would define a custom
analyzer that would insert such things, but we also want to allow
customization of the analyzer for other more usual purposes.  I suppose we
could create a "wrapping tokenizer" at least in Lucene world.

Query optimization w/text index

Predicates having one value constant and the other a node sequence (with a
query); find the trailing QName of the query and generate a
lux_node_<QName>:value query term.

what about //e[. = 'foo' or . = 'bar'] ?
what about //e[contains(.,'bar')] ?

We are lacking a formal calculus - it would be nice to be able to prove
these optimizations correct, and to be able to compute the optimizations
using some more rigorous formalism.

Define search{expression} to mean collection()[expression-query] where
expression-query is some filter that selects (at least) all the documents
satisfying the expression.  In general, 

search{expression}/expression <=> collection()/expression

and search{expression} <= collection()


For example we can always rewrite:

    PATH[FILTER]

as
    search{PATH/FILTER}/PATH[FILTER] 

if FILTER is a path

If a document $d satisfies a[path], ie exists($d/a[path]) is true()
Then $d[a[path]] is clear, and I think in general that is equivalent to
$d[a/path]? 

but if FILTER is some other expression, what then?

We can always rewrite a path containing Dot (.) as a query by inserting the
context expression in place of Dot.  EG:

/path[function(.)] can become

search{function(/path)}/path[function(.)]

Because /path[function(.)] => function(/path)

So this should lead to a practicable path-rewriting method that will enable
us to optimize predicate expressions containing (.) in a more uniform and
systematic way, rather than ad hoc optimizations for specific patterns.

So now the question is: in our expression tree, how do you get hold of the
context expression for a given expression?  Also: can we get a query for
the context expression without performing teh rewrite explicitly?

Basically, from any Dot, walking back up the tree until an absolute
expression: doc(), collection(), root(), Slash, accumulating paths, yields
the context expression