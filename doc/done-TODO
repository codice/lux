* tiny binary
In order to distinguish "regular" binary documents from XML binary documents, we can:
*** add a content-type field
lux_content_type; a stored string-valued field (DocValues?); should be
renameable, but doesn't require any solrconfig.xml or schema.xml changes
*** encode the "type" as a magic number (TINY) in the binary content itself
paves the way for a file format; adds 4 bytes to each document; decode document from offset 
position.  Has the virtue of being the same treatment as existing binaries.
* optimization fixes
** //foo[non-path-expr = 'foo']
was generating query(foo:foo); we now do this only for [. = 'foo']
** broken where clause opto:
for $book at $i in collection()
  where $book//section[@id="9780465008308"]
  return $i
always returns 1
** stopped queries from let clause expressions from bleeding
These were contributing to the filtering of enclosing expressions and could
cause some documents to be missed.
** paths with predicates referenced by variable
let $x := /a[b]/c 
gets an incorrect query attached to it (/a/b/c)
** Enabled the optimization of certain expressions including variables
variables bound in let expressions that is
*** we concatenate paths in variable expressions as well:
let $foo := /foo return $foo/bar should generate /foo/bar
** paths containing predicates generated incorrect queries
simple predicates worked, but paths like /a[b]/c would generate queries
like /a/b/c rather than a/b AND b/c
** generate IGNORABLE queries for non-isomorphic functions 
like string(); these queries are marked as IGNORABLE.  This has the effect
that if their query is the only one, it will act as a filter, otherwise it
is ignored.
** there was a bug generating queries for paths beginning collection()
We weren't pushing a query on the stack?
** mark subsequence (first or last) as SINGULAR
* enhancements
** distributed search (and indexing)
*** document identity and ordering
We have been using docid as an ordering mechanism until now.  Cloud
complicates this because the same docid may exist on multiple shards.  But
this problem merely exposed an underlying problem we already had w/Solr:

XQueries involving multiple Solr queries may get documents on either side
of a commit (maybe a soft commit).  In this case, documents may be added or
deleted, and existing documents may have their ids changed.  This causes
problems with joins involving document or node identity.

The only way to guarantee correct XQuery semantics in the face of updates
for such queries is to provide a stable global ordering for all documents.
We could do that easily enough using our existing unique key, uri.  However
we face an additional hurdle in that Saxon requires a numeric document id
for every document, which is what it uses to enforce document order.  Our
unique keys (uris) are strings though, and it's not possible to assign them
unique 64-bit ints; we'd also need to preserve the natural string order
since our id order must correspond with the order of our unique key.

Let's abandon the idea of using uri for ordering and consider the idea of
providing our own unique integer key.  This would require us to build an
atomically-updatable persistent id sequence somewhere and to assign ids on
insert.

But we're interested in providing a Solr Cloud implementation, which means
updates can come to multiple nodes simultaneously.  We would need to handle
coordination of updates to the atomic id sequence across any number of
nodes, and what this would amount to basically, would be adding a
transaction mechanism to Solr Cloud, and that seems like a tall order.

Now instead let's consider the consequences of relaxing the XQuery language
constraint.  We can concoct an integer id that mimics the "unordered" Solr
Cloud document order by merging a shard id and the (shard-local) document
id: (shard_id << 32) | docid.

If a commit occurs during the evaluation of a multi-query XQuery, we could
end up with:

a) an id collision: a different document in query B re-uses the id of a
document in query A.  This could cause us to retrieve document A from the
cache.

b) an id change: the same document in queries A and B with different ids:
these will appear as distinct documents, albeit with the same content.

Essentially, we will get incorrect results from queries of this sort.  And
preventing this is very difficult.  Our only real option is to provide some
sort of error detection and recovery.  We can easily detect id changes by
storing a per-query (ie short-lived) two-way map id <=> uri.  Then if we
detect either an existing id with a different uri, or an existing uri with
a different id, we can throw an error and attempt to recover.

Probably the only thing we can really do to recover if the document
ordering changes mid-query, is to restart the query and try again.  We can
do this without the user noticing anything other than a slowdown.  If
enough failures occur in the same query we would eventually throw an error.

So how bad is this?  Querying documents while they're being updated may
happen during the course of some kind of batch data update, but queries
that depend on node and document identity in XQuery are fairly unusual.
Oftentimes they can be rewritten as simpler predicates.  The worst thing
might be degraded performance that goes unnnoticed due to frequent query
resets.  We should log these at WARN level so users are aware of their
existence.

Implementation notes:

As far as maintaining the map goes: we already keep an id->document
(XdmNode) map, and the document includes both its uri and its id
(XdmNode.getDocumentNumber()).  We don't have a uri->document map though;
we would probably have to add that.  But the existing map (nodeCache) is an
LRUCache that is limited in size, so it wouldn't guarantee correctness once
a node was expired from the cache.  This is a guard againt blowing out the
heap, since it includes entire documents. So probably we would need to add
both a uri->id map and an id->uri map.

OK that's out the window now since we don't actually get docids back from
Solr, and hacking it to return them looks like a real PITA, aside from
which it's leaking internal stuff etc.

So looking into generating a unique long key: we could generate a fairly
unique key using uri.hashval() * time() * random() (or seed random with the
foregoing), but then we have to look up that id and insert only if it
doesn't already exist *in solr*.  But this isn't truly an atomic operation:
in theory someone else can insert our random key.  Unlikely, but this also
has the drawback of having to run a query across the cluster before every
update.  Another idea: we can just be optimistic about the uniqueness -
someone on SO calculated the likelihood of a collision in a 10M document
collection to be about 1/10M.  So that's quite unlikely, and then it's not
as if document integrity is compromised if we have a collision - just
document order in some unusual queries.

In the short term we can just assign ids sequentially for each query and
not worry about this, I guess.

So SignatureUpdateProcessor creates a (64-bit!) hash based on a document's
content.  It was designed for dedupe processing.  But if we simply feed it
the uri, it can serve as a source of a 64 bit integer -- we need to ensure
that the integer sorts the same way as the signature.  Of course there is a
potential for hash collisions.  A nice fix would be to mix in a random
number or timestamp field to the hash: that way if an insert fails due to
non-identical signatures, we just try again.

uniqueness: lucene deletes the document matching the "updateTerm" which can
either be an id field (like uri), or the signature field. In our case we
would use uri, I think.  Then, if idField != signatureField, the
updatehandler also checks for uniqueness of the signature, but what it does
is to delete any existing matching documents!  So this is not really helpful...

Perhaps another approach would be to extend SignatureUpdateProcessor to
query for the existence of a hash match with a different id (uri).  But
will this work with clouded updates?  Does this processor work at all
w/shards?  It appears to check for dupes only in a local index.

*** search
This is complicated b/c Saxon wants to pull results, and Solr, to push
them.  We had a similar issue in the single-node search with Lucene, but
managed to overcome by implementing our own pull-style Lucene searcher.
It's not truly pull-style when the results are sorted (by relevance score
or by value).  In that case we buffer the first N results (docids only),
and if more get pulled, we fetch the next batch.

In Solr Cloud, the RequestHandler orchestrates communication with a number
of shards, and with a list of SearchComponents.  It calls each component
once (distributedProcess()) in each phase.  Communication between these
components is mediated by the ResponseBuilder, which holds information
about the current state of the request.

Ultimately what we need to do is evaluate the xquery (control passes to
Saxon), and then when a search function is called, initiate the entire
distributed search process, following a similar procedure as we currently
do for sorted results in LuxSearcher: fetch the top N from each shard, and
repeat if needed.

This suggests tinkering with the RequestHandler.  XQueryComponent
can *call* RequestHandler.handleRequestBody and then return STAGE_DONE.
Now typically SearchComponents do not maintain a pointer to the handler
that owns them.  We maintain a (per-core) singleton, SolrIndexConfig, which
is available to the XQueryComponent via the core's plugin registry.
Probably we can grab onto the RequestHandler from there?  There can be
multiple request handlers though, and we want to make sure to use the same
one: so we can iterate over all the request handlers in the core, and
iterate over all the components in each handler until we find the one we're
attached to.  Alternatively we could subclass the RequestHandler and inform
the component (if it's an XQueryComponent)

** highlighting
** optimized sorting
** count(collection())
** simplified some path queries
We now collapse nested Boolean and Path queries when possible
eg (a AND (b AND c)) now becomes (a AND b AND c)
* better app server packaging
created a standalone app that embeds jetty so we can control uris,
redirects and other http stuff without relying on solr. This will also
create a way to hide Solr behind a secure layer.

* optimizing function definitions
This was easy - just needed to let the optimizer have access to the function body

* thread-safety
I tested using multi-threaded test runner and found some bugs. Notes about
concerns that should be addressed now since I made CachingDocReader thread
safe (by giving it a thread-local document id to allocate), and I believe
LuxSearcher was *already* thread-safe.

Some aspects of Saxon are difficult to make thread safe and efficient (lock
free): the CollectionURIResolver is shared at the Configuration level;
Processor/Configuration are advertised as thread-safe.  However our
resolver must be allocated per-thread in order to guarantee thread-safety.

URIResolver, CollectionURIResolver are set on Configuration, but need to
read documents from the index, so they use LuxSearcher and
CachingDocReader, both of which have per-request scope.

For example, in a multicore Solr setup, one Evaluator could have a Searcher
tied to one core, while another Evaluator could be tied to a completely
different core.  But there can only be a single CollectionURIResolver.

Perhaps the solution is to pool Processors among threads, then?

Or: make a CollectionURIResolver that *is* threadsafe and pulls the
thread-unsafe resource as needed from a pool?

** also found threading bugs related to OutputURIResolver
This is shared via saxon's Configuration and needed to be thread-safe - now it is, and
we have a good test to ensure it stays that way.

* query highlighting
XmlHighlighter works along the lines of the Lucene highlighter, but with some xml awareness.
It accepts a NodeInfo and a Query and produces a NodeInfo, via StAX event streams.
There is a pluggable class to do the actual highlighting.  All query types supported by 
the Lucene Highlighter are supported,
and use the standard analysis chain (so matching respects case-folding and so on).  
The one exception is element text phrase queries: these are highlighted without regard to 
element containment due to limitations in the Lucene highlighter phrase support, which works 
only on a single field at a time.
* improved diagnostics via Solr component
Now when there's a syntax or runtime error, you get a meaningful error
message on the client side.
* module import
* optimized sorting
** support ordering by named indexes

Create function lux:key(name as xs:string) returning item which accepts the
name of a lucene index. Then arrange for order by lux:key('foo') to order
by the (single) value of foo, and replace the funcall with constant true()

Create function lux:has-key(name as xs:string, value as item()) - add the
constraint key(name)=value to the prevailing search query and replace the
expression with the constant true()

Rename fieldTerms to lux:key-terms(name as xs:string) ?

** order by optimization
Compute the XPath of the order by expression and compare to known XPath
indexes Requires that index definitions be discovered, or registered?  Need
to make sure the index analysis is compatible with the order by collation
** empty least/greatest
** configuration
define keys in IndexConfiguration
** testing strategy
* Saxon PE/EE compatibility
** detect licensing status
We now detect which version of Saxon is installed and whether it is
licensed, and choose a search strategy based on that.
** implemented collection()-based querying
I did this - but it was a waste of time.  I realized about 80% of the way
through that I should have actually tested to see whether fn:collection()
was in fact marked as document-ordered (ORDERED_NODESET) by Saxon.  It
appears it wasn't, in spite of what M. Kay said.  Perhaps they'll decide
that was a bug and fix it, so I'll leave the code in place.

However some progress was made on this front by following through on
another of Michael Kay's comments: don't generate a document-ordered
expression if you don't need to.  So now we use flwor expressions in those
cases rather than path expressions.

Need to come up with a query dialect that we can generate and parse.  To
make the parsing tractable, use ExtendableQueryParser.  Added an extension
so you can indicate span queries.  To generate, add a toQueryString()
method to ParseableQuery.
*** TermPQuery, SpanTerm, and QNameTextQuery
can we consolidate?  One possible problem is QNTQ produces PhraseQuerys
which can't be embedded in Spans, which we use to encode XPaths.

However as it is now, we don't generate QNTQ embedded in spans. 

//foo/bar[. = 'big dog'] will convert into:

lux:path(1w( 99w(/ foo) bar)) AND lux_elt_text:("bar\:big bar\:dog")

We could even be more clever and generate:

lux:path(1w( 99w(/ foo) bar))
  AND lux_elt_text:("bar\:big bar\:dog")
  AND lux_elt_text:("foo\:big foo\:dog")
  AND lux_elt_text:("foo\:big bar\:dog")
  AND lux_elt_text:("bar\:big foo\:dog")

and we *still* wouldn't need to worry about the embedding issue.
**** QNameTextQuery
has term and qName (ie field, qName and text) Field can be one of:
lux_att_text, lux_elt_text, or lux_text generates a <QNameTextQuery> which,
when parsed, expands into either a TermQuery or a PhraseQuery.
**** TermPQuery
has term only: field is same as QNameTextQuery, but text is qName *only*
Generates a <TermsQuery>.  I don't think this is ever actually used.
**** SurroundTerm
just like TermPQuery - what do we need this for??
Generates a <SpanTerm>
* document updates via xquery
lux:insert, lux:delete, lux:commit
Note: these functions have no side-effects and return empty-sequence: in order to 
prevent Saxon from optimizing them away, you have to pretend to do something with their
result!
** solr
we need to coordinate with Solr's management of the index writer and
related readers.  In 3.X., Solr doesn't yet have NRT support, so we 
simply write documents and leave commit() up to the user.  IndexWriter is
managed by Solr's UpdateHandler. We create a DocumentWriter interface
which takes a uri and an XdmNode and insert one in the Evaluator, making it
available to lux.function.* Implement this using XmlIndexer and Solr
UpdateHandler, or, for direct Lucene usage and for tests (?), using
something wrapping an IndexWriter directly.
* code reorg
Massive re-org, re-naming, etc.
Untangled a lot of dependency cycles
Split unchanging parts of XmlIndexer into IndexConfiguration
Split Saxon into Compiler (mostly unchanging), and Evaluator (transient), but work remains here
** eliminated lux.api and lux.saxon
This is mostly done: still need to move these classes into the root (lux)
package.  Then check for more cycles w/Sonar.
* xslt
We want to use for document output and also for input transformations
provide as function from xquery: lux:transform()
** also lux:eval()
* app server
This works OK but requires some minor damage to solr. Might be better as a
separate lightweight app??
** Need to create a mini-app
In the ened we built an entirely separate app server, distinct from Solr.
Essentially it performs url rewriting since Solr has weird and inflexible
url conventions.  

map /*.xqy to XQueryServlet -- can we do this in jetty
webdefault
*** LuxSolrFilter
We need to build a Filter rather than (or in addition to a Servlet)
so we can siphon off xquery webapp requests before handing off to solr
This filter could also be used to shut off external access to Solr...
** solr QueryResponseWriter
that serializes the output of XQuery evaluation as HTML (and/or XML or
text).  Can configure a path should map to either a folder on the
filesystem, or in lux storage. Use this to deploy a mini-appserver at
/lux/index.xqy that demonstrates lux.
** rest service 
Portion of url following .xqy now mapped as "path-extra" as in CGI.  In the
demo, we use this to display documents from /view.xqy<uri>
** search demo
*** autocompletion of node names and words
using new lux:fieldTerms() method which uses a Lucene TermEnum.
**** Don't autocomplete when the user-entered term doesn't match
*** auto-select top term
Display auto-completed portion selected so typing overwrites it.  Nifty
javascript fun.
*** sunflower logo
I'm no artist but I like it.  ok - maybe try the circles version?  some
other thing that's in between?
*** more design love
some color and a logo font
*** pagination
*** result display
*** namespace-aware queries
using reverse clark names? or using in-scope prefixes
* query API
lux:search now accepts queries formatted as node trees using the Lucene XML
query parser scheme.  The indexes provided include node (QName) path
indexes and node word indexes.
** full text search *across the whole document*
eg lux:search("phrase to find") will match documents containing the phrase.
** full text search *spanning all descendants* of a node
eg //foo["phrase to find"] would match <foo><a>phrase<a> to <b>find</b></foo>
*** Note: elements introduce token boundaries
eg //foo["phrase to find"] would match <foo><a>phrase<a>to<b>find</b></foo>
* storage
TinyBinary yields 5-20x speedup in document read time.  Documents
are *larger* though - about 1.5x as big, with the best encoding so far.
Apparently 19 pointers per node is expensive.
** Separated TinyBinary from Lux trunk
This is our own binary storage format that reconstructs TinyTrees.  If we
change to a Saxon-compatible Configuration, then we could support the use
of PTree storage.  I've left the code on a branch in the lux distro; it
might be nice to provide it as a run-time plugin; we could load via
reflection?
** test run
Original size=361 bytes, 'tiny' binary size=833 bytes
DocBuilder: 907ms; TinyBinary: 116ms
Original size=361 bytes, 'tiny' binary size=731 bytes
DocBuilder: 663ms; TinyBinary: 55ms
Original size=361 bytes, 'tiny' binary size=833 bytes
DocBuilder: 597ms; TinyBinary: 18ms
Original size=361 bytes, 'tiny' binary size=731 bytes
DocBuilder: 530ms; TinyBinary: 21ms
Original size=288815 bytes, 'tiny' binary size=730310 bytes
DocBuilder: 6987ms; TinyBinary: 1852ms
Original size=288815 bytes, 'tiny' binary size=559768 bytes
DocBuilder: 6643ms; TinyBinary: 1205ms
** test w/VInt encoding
Original size=361 bytes, 'tiny' binary size=674 bytes
DocBuilder: 746ms; TinyBinary: 103ms
Original size=361 bytes, 'tiny' binary size=572 bytes
DocBuilder: 600ms; TinyBinary: 60ms
Original size=361 bytes, 'tiny' binary size=674 bytes
DocBuilder: 393ms; TinyBinary: 17ms
Original size=361 bytes, 'tiny' binary size=572 bytes
DocBuilder: 332ms; TinyBinary: 20ms
Original size=288815 bytes, 'tiny' binary size=645259 bytes
DocBuilder: 6520ms; TinyBinary: 2291ms
Original size=288815 bytes, 'tiny' binary size=474717 bytes
DocBuilder: 6349ms; TinyBinary: 1873ms
** test w/VInt and alpha-delta encoding
Original size=361 bytes, 'tiny' binary size=662 bytes
DocBuilder: 729ms; TinyBinary: 94ms
Original size=361 bytes, 'tiny' binary size=560 bytes
DocBuilder: 572ms; TinyBinary: 58ms
Original size=361 bytes, 'tiny' binary size=662 bytes
DocBuilder: 376ms; TinyBinary: 17ms
Original size=361 bytes, 'tiny' binary size=560 bytes
DocBuilder: 318ms; TinyBinary: 20ms
Original size=288815 bytes, 'tiny' binary size=608309 bytes
DocBuilder: 6454ms; TinyBinary: 2172ms
Original size=288815 bytes, 'tiny' binary size=437767 bytes
DocBuilder: 6239ms; TinyBinary: 1658ms
** test w/beta, namespace coding, utf-8 for strings
Original size=361 bytes, 'tiny' binary size=567 bytes
DocBuilder: 943ms; TinyBinary: 121ms
Original size=361 bytes, 'tiny' binary size=390 bytes
DocBuilder: 637ms; TinyBinary: 127ms
Original size=361 bytes, 'tiny' binary size=567 bytes
DocBuilder: 488ms; TinyBinary: 20ms
Original size=361 bytes, 'tiny' binary size=390 bytes
DocBuilder: 437ms; TinyBinary: 25ms
Original size=288815 bytes, 'tiny' binary size=512618 bytes
DocBuilder: 8215ms; TinyBinary: 2512ms
Original size=288815 bytes, 'tiny' binary size=341947 bytes
DocBuilder: 8074ms; TinyBinary: 1677ms

* optimizations
** basic indexing support: qname retrieval speeds queries when the names are selective
** count() is optimized for a special case:
when the query for the argument can be shown to have the same cardinality;
basically this is when the argument is a simple path expression equivalent
to these forms:

//element/root()
//element/ancestor::document-node()
(/)[.//element]

etc..
*** also in that case, when not counting (just returning documents),
we save some time by not executing the xpath.
** exist(), empty()
short-circuit counting; exits search as soon as a doc is found. not() is
handled as well since saxon converts not(node-sequence) into
not(exists(node-sequence)).
** count(//a) + count(//b)
** lazy result iteration
Rather than trying to figure out in advance how many results will be
required, allow the caller to pull results lazily.  Tested this with exists()
and empty(), and with count() > N.
*** subsequence(); [1], etc.
This sometimes optimizes properly, but not in every case it could.  There
are difficulties because of the need to preserve document ordering.
** document identity
Need to cache documents, at least for the duration of a single query, so we
don't recreate the same document twice: otherwise Saxon thinks they are
different.
** predicate submersion
Express paths returning documents (say ending with root()) as
search()[path] so that the path doesn't need to be sorted in document
order, enabling lazy evaluation.
*** This does seem to have worked
I found this note in impl-TODO, but at the moment we don't seem to have any
problem like this (see SearchTest.pathOrder which retrieves 120 nodes from
6 documents in //SCENE/root()//ACT).  It seems to have gotten sorted out
when we implemented path indexes, although I'm not sure why since we still
emit a purely qname-based query: SCENE AND ACT.

**** removed lux:root()
subsumed by predicate optimization

**** note about root()
Background: the evaluator has to ensure that sequences of nodes are unique
and in document order.  Our Optimizer calculates and asserts document
order, but doesn't detect when there are multiple copies of the same node.
This can occur due to doc() and root().  Maybe other functions?  A sequence
like //foo/root() returns documents, but the evaluator iterates over nodes
(//foo) in each document, and returns each document once for each
occurrence of foo in its document.
** optimize contains()
** predicates with atomic/node comparisons 
like [foo = "foo"]
* indexes
XmlIndexer manages multiple index options, and we pass it around so consumers
of the index know what is available there.
** qname index
** path index
** value indexes
index key: <QName> | <first N chars of value>
we could have path value indexes too:
{} <QName> <QName> ... | <first N chars>
in fact this index can probably just replace the QName or path index
** word indexes
Now works using single fulltext field with terms that incorporate the field
name so we can do near queries across different element names, and
implement transparent elements (like ML phrase-through).
*** QNameAttribute / Impl / Factory / Filter
We provide a QName-based analysis chain wrapped around StandardTokenizer /
LowerCaseFilter.
*** create QNameTextTokenStream
returned by QNameTextField
*** Adding character offsets
so we can support highlighting.  We also need this for compatibility
w/ifpress - so we don't need two separate xml_text fields Compared
parsing/indexing speed with and without offsets: there was a 3% slowdown in
the overall time to index when offsets were added (see IndexTest below).
**** Measured indexing speed
IndexTest.testIndexFullTest 
Without offsets: 4481 4690 4677 4637 4576
With offsets: 4752 4244 4740 4891 4873
throwing out the min and max, averaging, the difference is about 150 ms, or
about 3% difference.
** user search function
Supporting full text queries; we do have lux:search (xs:string); however
the query syntax is bizarre since you must specify an element name for
every token using eltname{namespace}:word.  Conversely we have lux:search
(element()), but no xquery helper functions for constructing the queries.
However that seems to be the way to go.
*** query syntax
**** lux:query("element", "value")
***** shorthand: :element:value
**** lux:query("prefix:element", "value")
uses namespace bindings declared in the query
***** shorthand: prefix:element:value
**** lux:query("@attribute", "value")
***** shorthand: :@attribute:value
**** lux:query("element/@attribute", "value")
just a simple and-query of "element/@attribute" and "@attribute=value"
***** shorthand: :element/@attribute:value
**** lux:and(), lux:or(), lux:not()
*** modified string query parser
standard lucene query parser syntax where field names may contain "@" and
namespace *prefixes*.  Post-process to prefix terms with element/attribute
names.
* tests
** testing doc() and collection()
** search test coverage
Run search tests for everything in query test, comparing "raw" against
indexed results.
** XQuery Test Suite
** XQuery!
running XQTS now - down to 1305 failures out of 12008 tests over a few days
we got it down to 147 w/44 ignored.  Now 7 tests fail.  
*** 1 is a test harness glitch
*** 1 is due to no schema support
*** 3 are due to no support for type in:
element(foo,xs:type) 
*** 2 are due to an unexplained compile problem with the context item (.)
Then, running on a larger test suite: Ran 18623 tests; 29 tests fail -
mostly these are related to collection() and a few unicode surrogate
issues.
* lux components
** Translator
converts Saxon's internal Expression tree to a Lux AbstractExpression tree.
** PathOptimizer
optimizes an AbstractExpression tree using Lucene searches with element and
attribute name indexes.
*** ExpressionVisitor
machinery for optimizers and other tree visitors like UnOptimizer.
** Indexer
*** base-uri
We now attach a base uri to each result document
** Lux Function Library for XPath/XQuery 
lux:search, lux:count, lux:exists execute a lucene query, returning
documents, count, and boolean respectively, which are then processed by the
evaluator as part of an enclosing XPath or XQuery expression. Currently
used only internally by the optimizer, but we intend to provide
user-callable versions as well supporting some form of lucene query syntx.
*** optimize absolute (rooted) context-free expressions
The optimizer considers inserting one of the lux function whenever such a
sub-expression occurs.
** Solr integration
UpdateProcessor and SearchComponent
** XQuery Evaluator
* packaging
** solr request handler
restored to working order
** MPL license
* code cleanup
** reorganized class/package names
moved stuff out of lux package into lux.search; created lux.query for
queries.
** shorten xml index field names?
to make it easier to type queries.  Maybe make the default field name be
something reasonable, like xml_text?  then use lux_att_name, lux_elt_name,
lux_path?  This is fine, but requires schema changes for the Solr version.
* bug fixes
** query quoting
quote operators like and, or, not, n, w.  If we have these as QNames, we'll
fail!  Also - quote quotes, etc.  These could appear in namespace uris, eg.
Maybe we did that part already?
** trailing funcall()
was being treated as root()
